#explain command
k explain pv --recursive

#deploy application in cluster
kubectl run hello-minikube

kubectl run nginx --image nginx

#label
kubectl run redis --image redis:alpine -l tier=db

kubectl run custom-nginx --image=nginx --port=8080

#dry run
kubectl run redis --image redis123 --dry-run=client -o yaml > pod.yaml

kubectl apply -f pod.yaml

#view info about cluster
kubectl cluster-info

#get nodes
kubectl get nodes

#create pods
kubectl create -f pod-definition.yml

#get pods
kubectl get pods

kubectl get pods -o wide

kubectl get pod redis -o yaml > pod-definition.yaml

#describe pod
kubectl describe pod myapp-prod

#delete pod
kubectl delete pod webapp

#delete all
kubectl delete all --all

#edit pod
kubectl edit pod redis
:wq then Enter

#replication controller

kubectl get replicationcontroller / rc
kubectl get replicaset / rs

#scaling

kubectl replace -f replicaset-definition.yml (adding in yml file)

kubectl scale --replicas=6 -f replicaset-definition.yml

#delete replica set

kubectl delete replicaset myapp-replicaset

#get all objects

kubectl get all

#create deployment

kubectl create deployment httpd-frontend --image=httpd:2.4-alpine
kubectl scale deployment --replicas=3 httpd-frontend

#autoscale
kubectl autoscale deploy nginx --min=5 --max=10 --cpu-percent=80

k delete hpa nginx

#Formatting outputs

-o json
-o name
-o wide
-o yaml

#namespaces

kubectl get ns
kubectl get pods --namespace=kube-system

kubectl create -f pod-definition.yml --namespace=dev

1. create a namespace-dev.yml to create a namespace
 namespace:dev can be added into metadata section

2. kubectl create namespace dev

#set name space permanently

kubectl config set-context $(kubectl config current-context) --namespace=default
kubectl get pods

kubectl get pods --all-namespaces
k get po -A
kubectl -n research get pods

#service

kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml
kubectl expose pod redis --name redis-service --port=6379 --target-port=6379
kubectl run httpd --image=httpd:alpine  --expose --port 80

#docker

docker run ubuntu
docker ps
docker ps -a

docker run ubuntu sleep 5

#DockerFile

FROM Ubuntu

#argument will be appended to docker command
ENTRYPOINT["sleep"] 

#default argument
CMD["5]

CMD sleep 5
CMD["sleep","5"]

#Docker to Kubernetes Mapping

ENTRYPOINT["sleep"] -> commands:["sleep2.0"]
CMD["5"] -> args: ["10"]

- command field in kubernetes pod file overwrites the ENTRYPOINT command in docker file
- args field in kubernetes pod file overwrites the CMD command in docker file

#env
k exec webapp-color -- printenv

#ConfigMaps

kubectl create configmap app-config --from-literal=APP_COLOR=blue --from-literal=APP_MOD=prod

kubectl create configmap app-config --from-file=app_config.properties

kubectl get configMaps (cm)

Kubectl describe configMaps <configmap.name> 

#Secrets

kubectl create secret generic db-secret --from-literal=DB_Host=mysql --from-literal=DB_User=root

kubectl create secret generic db-secret --from-file=app_secret.properties

kubectl get secret db-user-pass -o jsonpath='{.data}'

#encode
echo -n 'mysql' | base64

#decode
echo -n 'bxhdyd' | base64 -- decode

kubectl get secrets

kubectl describe secrets app-secret

kubectl get secret app-secret -o yaml > secret.yaml

#Docker Security

- We can choose to have security in container level or pod level
- Settings on container will override the settings on pod

#pod level

#check user on which the pod process is executed	
kubectl exec -it ubuntu-sleeper -- whoami	

spec:
	securityContext:
		runAsUser: 1000
		
#Container level

spec:
	containers:
		- name: ubuntu
		  image: ubuntu
          securityContext:
			runAsUser: 1000	
            capabilties:
				add: ["MAC_ADMIN"]
				
#Service Account

kubectl create serviceaccount dashboard-sa				

kubectl get serviceaccount

- a token is created as a secret object

#token
kubectl describe serviceaccount dashboard-sa

kubectl describe secret dashboard-sa-token-kbbdm

#Check Default Service Account
kubectl exec -it httpd ls  /var/run/secrets/kubernetes.io/serviceaccount

-delete and recreate the pod

spec:
	containers:
	serviceAccount: dashboard-sa
    automountServiceAccountToken: false

#Resource Quota

k create resourcequota myrq --hard=cpu=1,memory=1G,pods=2
k get resourcequota

#Recource Requests

spec:
	containers:
	resources:
	  requests:
		memory: "1Gi"
		cpu: 1
	  limits:
        memory: "2Gi"
		cpu: 2
		
#Taint and Tolerations		

kubectl taint nodes node-name key=value:taint-effect

#taintEffect
1. NoSchedule
2. PreferNoSchedule
3. NoExecute

kubectl taint nodes node1 app=blue:NoSchedule

#Search for Taint
kubectl describe node kubemaster | grep Taint

#Remove the Taint
kubectl taint nodes master/controlplane node-role.kubernetes.io/master:NoSchedule-

#Tolerations

spec:
	containers:
    tolerations:
	- key: app
	  operator: "Equal"
	  value: "blue"
	  effect: "NoSchedule"
	  
#NodeSelector

kubectl label nodes node-1 size=Large

spec:
	containers:
	nodeSelector:
	 size: Large
	
#NodeAffinity

spec:
	containers:
	affinity:
		nodeAffinity:
		 requiredDuringSchedulingIgnoredDuringExecution:
		  nodeSelectorTerms:
		  - matchExpressions:
			- key: size
			  operator: In (NotIn)
			  values:
			  - Large		
			  - Medium		  
		  
#Operator values
In, NotIn, Exists etc...

#Multi Container Pods

spec:
	containers:
	- name: webapp
	  image:
	- name: log-agent
      image:	

#access log file in a running container
kubectl exec -it app cat log/app.log -n elastic-stack

#Readiness Probe

readinessProbe:
	httpGet:
		path: /api/ready
		port: 8080
		
livenessProbe:
	httpGet:
		path: /api/ready
		port: 8080	
    initialDelaySeconds: 10
    periodSeconds: 5
    failureThreshold: 8

#get all events by namespace
kubectl get ns # check namespaces
kubectl -n qa get events | grep -i "Liveness probe failed"

#logging
kubectl logs -f event-simulator-pod	

kubectl logs -f event-simulator-pod	container-name

k delete po busybox --force --grace-period=0

#metrics server

minikube addons enable metrics-server

kubectl create -f deploy/1.8+/

kubectl create -f .

kubectl top node

kubectl top pod

--------------------------------------------------------------------------------
#labels

metadata
  name:
  labels:
    app: App1
	function: Front-end
  annotations:
    buildVersion: 1.34  
	
k get po --show-labels
k labels po app=v3 --overwrite
k get po -L app
k label po nginx2 nginx3 app-

#Annotations
kubectl annotate po nginx1 nginx2 nginx3 description='my description'

#Selectors	
kubectl get pods --selector app=App1	
kubectl get all --selector env=prod
kubectl get po --selector bu=finance,tier=frontend,env=prod

#rollout
kubectl create -f deployment-definition.yml

#record
kubectl create -f deployment-definition.yaml --record

kubectl get deployments

#by the label of deployment name(nginx)
kubectl get po -l app=nginx

kubectl apply -f deployment-definition.yml
kubectl set image deployment/myapp-deployment nginx=nginx:1:9:1

#rollout
kubectl rollout status deployment/myapp-deployment
kubectl rollout history deployment/myapp-deployment

#rollout revision
kubectl rollout history deployment/myapp-deployment --revision=1

#undo changes
kubectl rollout undo Deployment/myapp-deployment

#pause/resume deployment
kubectl rollout pause deploy nginx
kubectl rollout resume deploy nginx


#Jobs

apiVersion: batch/v1
kind: Job
metadata:
 name: math-add-job
spec:
  template:
    spec:
	  containers:
	   - name: math-add
	     image: ubuntu
		 command: ['expr','3',+','2']
      restartPolicy: Never		 
	  
#get jobs	  
kubectl get jobs

kubectl logs math-add-job-qb79q 

kubectl delete job math-add-job-qb79q 

kubectl create job throw-dice-job --image kodecloud/throw-dice --dry-run=client -o yaml > job.yaml

kubectl explain job --recursive | less

kubectl explain job --recursive | grep parallelism

#cronjob

kubectl get cronjob

kubectl create cronjob throw-dice-cronjob --image=nginx --schedule "30 21 * * * " --dry-run=client -o yaml > cronjob.yaml

--------------------------------------------------------------------------------

#service


kubectl create -f service-definition.yaml

kubectl get services

#scan for open ports
nc -z -v -w 1 10.10.8.8:80

k get endpoints foo

#NodePortService

apiVersion: v1
kind: Service
metadata:
 name: myapp-service
spec:
 type: NodePort
 ports: 
  - targetPort: 80
    port: 80
	nodePort: 30008
 selector:
	app: myapp
	type: front-end
	
#ClusterIPService

apiVersion: v1
kind: Service
metadata:
 name: back-end
spec:
 type: ClusterIP
 ports: 
  - targetPort: 80
    port: 80
 selector:
	app: myapp
	type: back-end
	
	
#Ingress

#Resource

#single route
apiVersion: extensions/v1betal
kind: ingress
metadata:
   name: ingress-wear
spec: 
 backend:
     serviceName: wear-service
	 servicePort: 80

#multiple routes
apiVersion: extensions/v1betal
kind: ingress
metadata:
   name: ingress-wear-watch
spec:
   rules:
	 - http:
	    - paths: /wear
		  backend:
		   serviceName: wear-service
		   servicePort: 80
		- paths: /watch
		  backend:
		   serviceName: watch-service
		   servicePort: 80  

#Splitting by host

apiVersion: apps/v1
kind: ingress
metadata:
   name: ingress-wear-watch
spec:
   rules:
   - host: wear.online.com
	 http:
	    paths: 
		  - backend:
		    serviceName: wear-service
		    servicePort: 80
   - host: wear.online.com		
     http: 
	    paths: 
		  - backend:
		    serviceName: watch-service
		    servicePort: 80
			
alias k=kubectl
kubectl create -f ingress-wear.yaml
kubectl get ingress
kubectl get ingress ingress-wear-watch --namespace app-space

kubectl get roles,rolebindings --namespace ingress-space

kubectl -n ingress-space expose deployment ingress-controller --name ingress --port 80 --target-port 80 --type NodePort --dry-run=client -o yaml > ingress-service.yaml


#Network Policy

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
   name: db-policy
spec: 

Ingress 3 sources

1. podSelector
2. nameSpaceSelector
3. ipBlock

kubectl get networkpolicy

##policy
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
  - Egress
  - Ingress
  ingress:
    - {}
  egress:
  - to:
    - podSelector:
        matchLabels:
          name: mysql
    ports:
    - protocol: TCP
      port: 3306

  - to:
    - podSelector:
        matchLabels:
          name: payroll
    ports:
    - protocol: TCP
      port: 8080
  
nc -z -v -w 1 secure-service 80

#Volumes

volumes:
- name: data-volumes
  hostPath:
   path: /data
   type: Directory
   
#AWS   
volumes:
- name: data-volumes
  awsElasticBlockStore: 
   volumeID: id
   fsType: ext4   
   
#mount

spec:
  containers:
    volumeMounts:
	 - mountPath: /opt
	   name: data-volume
	   
#Persistent Volumes

apiVersion: v1
kind: PersistentVolume
metadata: 
  name: pv-vol1
spec:
  accessModes:
    - ReadWriteOnce
  capacity:
   storage: 1Gi 
  hostPath: /tmp/data   
	
	
#Supported Modes

- ReadOnlyMany
- ReadWriteOnce
- ReadWriteMany

kubectl create -f pv-definition.yaml
kubectl get pv

#Persistent Volume Claim (pvc)

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
   - ReadWriteOnce
  resources:
    requests:
	 storage: 500Mi
	 
k create -f pvc-definition.yaml
k get pvc
k delete pvc myclaim

#context
k config get-contexts
k config current-context
k config use-context context-name   
   